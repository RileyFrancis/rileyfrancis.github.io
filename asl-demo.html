<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>ASL Live Demo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- TF.js (match converter major version) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>

  <!-- MediaPipe Hands + camera utils + drawing utils -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <style>
    body {
      margin: 0;
      height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
      background: #0f172a;
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      color: #e5e7eb;
    }
    .container {
      display: flex;
      flex-direction: column;
      gap: 1rem;
      align-items: center;
      padding: 1.5rem;
      background: #020617;
      border-radius: 1rem;
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.6);
      max-width: 960px;
      width: 100%;
    }
    h1 {
      margin: 0;
      font-size: 1.6rem;
    }
    .video-wrapper {
      position: relative;
      width: 640px;
      max-width: 100%;
      border-radius: 0.75rem;
      overflow: hidden;
      border: 1px solid #1f2937;
    }
    #output-canvas {
      width: 100%;
      display: block;
    }
    #video {
      display: none; /* we only show the canvas overlay */
    }
    .status-row {
      display: flex;
      justify-content: space-between;
      width: 100%;
      max-width: 640px;
      gap: 1rem;
      flex-wrap: wrap;
    }
    .status-box {
      flex: 1 1 200px;
      background: #020617;
      border-radius: 0.75rem;
      padding: 0.75rem 1rem;
      border: 1px solid #1f2937;
    }
    .status-label {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: #9ca3af;
      margin-bottom: 0.25rem;
    }
    #prediction-letter {
      font-size: 2.5rem;
      font-weight: 700;
    }
    #prediction-confidence {
      font-size: 1rem;
      color: #9ca3af;
    }
    #hand-status {
      font-size: 0.95rem;
    }
    #fps {
      font-size: 0.95rem;
    }
    .bar-row {
      display: flex;
      align-items: center;
      font-size: 0.8rem;
      margin-bottom: 3px;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
    }
    .bar-label {
      width: 1.5rem;
      color: #9ca3af;
    }
    .bar-bg {
      flex: 1;
      height: 8px;
      background: #111827;
      border-radius: 999px;
      margin: 0 0.35rem;
      overflow: hidden;
    }
    .bar-fill {
      height: 100%;
      background: #22c55e;
      width: 0%;
    }
    .bar-val {
      width: 3rem;
      text-align: right;
      color: #9ca3af;
    }
    #probabilities {
      max-height: 180px;
      overflow-y: auto;
      margin-top: 0.35rem;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>ASL Letter Classifier (Browser Demo)</h1>

    <div class="video-wrapper">
      <video id="video" autoplay playsinline></video>
      <canvas id="output-canvas"></canvas>
    </div>

    <div class="status-row">
      <div class="status-box">
        <div class="status-label">Prediction</div>
        <div id="prediction-letter">–</div>
        <div id="prediction-confidence">–</div>
      </div>
      <div class="status-box">
        <div class="status-label">Status</div>
        <div id="hand-status">Loading model…</div>
        <div id="fps">FPS: –</div>
      </div>
    </div>

    <div class="status-box" style="max-width: 640px; width:100%;">
      <div class="status-label">Class Probabilities</div>
      <div id="probabilities"></div>
    </div>
  </div>

  <script>
    // ===== CONFIG =====
    const MODEL_URL = "./web_model_fp16/model.json";  // adjust if path differs
    const CLASS_NAMES = [
      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',
      'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',
      'S', 'T', 'U', 'V', 'W', 'X', 'Y'
    ];
    const IMG_SIZE = 256;
    const PREDICTION_INTERVAL_MS = 120; // throttle inference

    // Same normalization as in your PyTorch code
    const NORM_MEAN = [0.485, 0.456, 0.406];
    const NORM_STD  = [0.229, 0.224, 0.225];

    // ===== DOM ELEMENTS =====
    const videoEl = document.getElementById("video");
    const canvasEl = document.getElementById("output-canvas");
    const ctx = canvasEl.getContext("2d");

    const letterEl = document.getElementById("prediction-letter");
    const confEl   = document.getElementById("prediction-confidence");
    const handStatusEl = document.getElementById("hand-status");
    const fpsEl    = document.getElementById("fps");
    const probContainer = document.getElementById("probabilities");

    // Hidden canvas for cropping
    const cropCanvas = document.createElement("canvas");
    const cropCtx = cropCanvas.getContext("2d");
    cropCanvas.width = IMG_SIZE;
    cropCanvas.height = IMG_SIZE;

    let model = null;
    let lastPredictionTime = 0;
    let lastFrameTime = performance.now();
    let fps = 0;

    async function loadModel() {
      handStatusEl.textContent = "Loading TensorFlow.js model…";
      model = await tf.loadGraphModel(MODEL_URL);
      handStatusEl.textContent = "Model loaded. Initializing camera…";
      console.log("TF.js model loaded", model);
    }

    function setupHands() {
      const hands = new Hands({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
      });

      hands.setOptions({
        maxNumHands: 1,
        modelComplexity: 1,
        minDetectionConfidence: 0.6,
        minTrackingConfidence: 0.5,
      });

      hands.onResults(async (results) => {
        const now = performance.now();
        const dt = now - lastFrameTime;
        lastFrameTime = now;
        fps = 1000 / dt;
        fpsEl.textContent = `FPS: ${fps.toFixed(1)}`;

        // Resize canvas to match video
        if (canvasEl.width !== videoEl.videoWidth || canvasEl.height !== videoEl.videoHeight) {
          canvasEl.width = videoEl.videoWidth;
          canvasEl.height = videoEl.videoHeight;
        }

        // Draw the video frame
        ctx.save();
        ctx.clearRect(0, 0, canvasEl.width, canvasEl.height);
        ctx.drawImage(results.image, 0, 0, canvasEl.width, canvasEl.height);

        // No hands?
        if (!results.multiHandLandmarks || results.multiHandLandmarks.length === 0) {
          handStatusEl.textContent = "No hand detected";
          letterEl.textContent = "–";
          confEl.textContent = "–";
          ctx.restore();
          return;
        }

        // Use first detected hand
        const landmarks = results.multiHandLandmarks[0];

        // Draw landmarks
        drawConnectors(ctx, landmarks, HAND_CONNECTIONS, {color: '#ffffff', lineWidth: 2});
        drawLandmarks(ctx, landmarks, {color: '#ef4444', lineWidth: 1});

        // Compute bounding box in canvas coordinates
        let xs = landmarks.map(lm => lm.x * canvasEl.width);
        let ys = landmarks.map(lm => lm.y * canvasEl.height);
        let xmin = Math.min(...xs);
        let xmax = Math.max(...xs);
        let ymin = Math.min(...ys);
        let ymax = Math.max(...ys);

        const pad = 20;
        xmin = Math.max(0, xmin - pad);
        ymin = Math.max(0, ymin - pad);
        xmax = Math.min(canvasEl.width,  xmax + pad);
        ymax = Math.min(canvasEl.height, ymax + pad);

        const boxW = xmax - xmin;
        const boxH = ymax - ymin;

        // Draw bounding box
        ctx.strokeStyle = "#22c55e";
        ctx.lineWidth = 2;
        ctx.strokeRect(xmin, ymin, boxW, boxH);

        ctx.restore();

        handStatusEl.textContent = "Hand detected";

        // Throttle predictions
        if (!model) return;
        if (now - lastPredictionTime < PREDICTION_INTERVAL_MS) return;
        lastPredictionTime = now;

        if (boxW <= 2 || boxH <= 2) return;

        // Crop from main canvas -> cropCanvas
        cropCtx.clearRect(0, 0, IMG_SIZE, IMG_SIZE);
        cropCtx.drawImage(
          canvasEl,
          xmin, ymin, boxW, boxH,
          0, 0, IMG_SIZE, IMG_SIZE
        );

        // Run classification
        tf.engine().startScope();
        try {
          let img = tf.browser.fromPixels(cropCanvas).toFloat().div(255.0); // [H,W,3]

          // Normalize: (x - mean) / std (broadcast)
          const offset = tf.tensor1d(NORM_MEAN).reshape([1, 1, 3]);
          const scale  = tf.tensor1d(NORM_STD).reshape([1, 1, 3]);
          img = img.sub(offset).div(scale);

          const input = img.expandDims(0); // [1, 256, 256, 3]

          // GraphModel: single input, single output
          const logits = model.execute({ "input": input }); // or model.execute(input)

          // logits shape: [1, 24]
          const probsTensor = tf.softmax(logits);
          const probs = probsTensor.dataSync(); // Float32Array length 24

          // Argmax
          let maxIdx = 0;
          let maxVal = -Infinity;
          for (let i = 0; i < probs.length; i++) {
            if (probs[i] > maxVal) {
              maxVal = probs[i];
              maxIdx = i;
            }
          }

          const letter = CLASS_NAMES[maxIdx] || "?";
          const confidence = maxVal * 100;

          letterEl.textContent = letter;
          confEl.textContent = `${confidence.toFixed(1)}%`;

          // Update probability bars
          renderProbabilityBars(probs);
        } catch (err) {
          console.error("Inference error:", err);
          handStatusEl.textContent = "Error during prediction (see console)";
        } finally {
          tf.engine().endScope();
        }
      });

      return hands;
    }

    function renderProbabilityBars(probs) {
      probContainer.innerHTML = "";
      for (let i = 0; i < CLASS_NAMES.length; i++) {
        const row = document.createElement("div");
        row.className = "bar-row";

        const label = document.createElement("div");
        label.className = "bar-label";
        label.textContent = CLASS_NAMES[i];

        const barBg = document.createElement("div");
        barBg.className = "bar-bg";

        const barFill = document.createElement("div");
        barFill.className = "bar-fill";
        barFill.style.width = (probs[i] * 100).toFixed(1) + "%";

        barBg.appendChild(barFill);

        const val = document.createElement("div");
        val.className = "bar-val";
        val.textContent = (probs[i] * 100).toFixed(1) + "%";

        row.appendChild(label);
        row.appendChild(barBg);
        row.appendChild(val);
        probContainer.appendChild(row);
      }
    }

    async function init() {
      await loadModel();

      // Start webcam
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480 },
        audio: false
      });
      videoEl.srcObject = stream;

      await new Promise(resolve => {
        videoEl.onloadedmetadata = () => {
          videoEl.play();
          resolve();
        };
      });

      canvasEl.width = videoEl.videoWidth;
      canvasEl.height = videoEl.videoHeight;

      const hands = setupHands();

      // Use MediaPipe Camera helper to feed frames into Hands
      const camera = new Camera(videoEl, {
        onFrame: async () => {
          await hands.send({ image: videoEl });
        },
        width: videoEl.videoWidth,
        height: videoEl.videoHeight,
      });

      camera.start();
      handStatusEl.textContent = "Running (show a hand to the camera)";
    }

    window.addEventListener("load", () => {
      init().catch(err => {
        console.error(err);
        handStatusEl.textContent = "Failed to initialize (see console)";
      });
    });
  </script>
</body>
</html>
