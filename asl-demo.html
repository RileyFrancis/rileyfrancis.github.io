<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>ASL Letter Classifier</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <!-- TF.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>

  <!-- MediaPipe Hands -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

  <style>
    body {
      margin: 0;
      background: #0f172a;
      color: #e5e7eb;
      font-family: system-ui, sans-serif;
      display: flex;
      justify-content: center;
      padding: 2rem 1rem;
    }

    .container {
      background: #020617;
      border-radius: 1rem;
      padding: 1.5rem;
      max-width: 1200px;
      width: 100%;
      box-shadow: 0 20px 40px rgba(0,0,0,0.6);
    }

    h1 {
      margin: 0 0 1rem 0;
      text-align: center;
      font-size: 1.6rem;
    }

    .main-row {
      display: flex;
      flex-wrap: wrap;
      gap: 1.5rem;
      justify-content: center;
    }

    .video-wrapper {
      position: relative;
      width: 640px;
      max-width: 100%;
      flex: 1 1 640px;
      border-radius: .75rem;
      overflow: hidden;
      border: 1px solid #1f2937;
      background: #020617;
    }

    #output-canvas {
      width: 100%;
      display: block;
    }

    #video {
      display: none; /* hidden, we draw via canvas */
    }

    .prediction-column {
      flex: 1 1 320px;
      max-width: 380px;
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }

    .status-box {
      background: #020617;
      border: 1px solid #1f2937;
      padding: 0.75rem 1rem;
      border-radius: .75rem;
    }

    .status-label {
      font-size: .8rem;
      letter-spacing: .08em;
      color: #9ca3af;
      margin-bottom: .25rem;
      text-transform: uppercase;
    }

    #prediction-letter {
      font-size: 2.5rem;
      font-weight: 700;
      line-height: 1;
    }

    #prediction-confidence {
      font-size: 1rem;
      color: #9ca3af;
    }

    #probabilities {
      margin-top: 0.35rem;
      /* no max-height / scrollbar: let it grow */
    }

    .bar-row {
      display: flex;
      align-items: center;
      margin-bottom: 4px;
      font-family: monospace;
    }

    .bar-label {
      width: 1.5rem;
      color: #9ca3af;
    }

    .bar-bg {
      flex: 1;
      height: 8px;
      background: #111827;
      border-radius: 999px;
      margin: 0 .35rem;
      overflow: hidden;
    }

    .bar-fill {
      height: 100%;
      background: #22c55e;
      width: 0%;
    }

    .bar-val {
      width: 3rem;
      text-align: right;
      color: #9ca3af;
    }

    /* Loading overlay inside video wrapper */
    #loading-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      background: rgba(15,23,42,0.85);
      z-index: 10;
    }

    .spinner {
      width: 40px;
      height: 40px;
      border-radius: 999px;
      border: 3px solid #334155;
      border-top-color: #22c55e;
      animation: spin 0.9s linear infinite;
      margin-bottom: 0.75rem;
    }

    .loading-text {
      font-size: 0.95rem;
      color: #e5e7eb;
      text-align: center;
      max-width: 260px;
    }

    @keyframes spin {
      from { transform: rotate(0deg); }
      to   { transform: rotate(360deg); }
    }

    @media (max-width: 900px) {
      .main-row {
        flex-direction: column;
        align-items: center;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>ASL Letter Classifier (Browser Demo)</h1>

    <div class="main-row">

      <!-- LEFT: video + loading overlay -->
      <div class="video-wrapper">
        <video id="video" autoplay playsinline></video>
        <canvas id="output-canvas"></canvas>

        <div id="loading-overlay">
          <div class="spinner"></div>
          <div class="loading-text">
            Loading model. This could take a minute…
          </div>
        </div>
      </div>

      <!-- RIGHT: predictions -->
      <div class="prediction-column">
        <div class="status-box">
          <div class="status-label">Prediction</div>
          <div id="prediction-letter">–</div>
          <div id="prediction-confidence">–</div>
        </div>

        <div class="status-box">
          <div class="status-label">Status</div>
          <div id="hand-status">Loading model. This could take a minute…</div>
          <div id="fps">FPS: –</div>
        </div>

        <div class="status-box">
          <div class="status-label">Class Probabilities</div>
          <div id="probabilities"></div>
        </div>
      </div>
    </div>
  </div>

  <script>
    // ===== CONFIG =====
    const MODEL_URL = "./web_model_fp16/model.json";
    const CLASS_NAMES = [
      'A','B','C','D','E','F','G','H','I',
      'K','L','M','N','O','P','Q','R',
      'S','T','U','V','W','X','Y'
    ];
    const IMG_SIZE = 256;

    // Same normalization as live_predict.py
    const NORM_MEAN = [0.485, 0.456, 0.406];
    const NORM_STD  = [0.229, 0.224, 0.225];

    // Same hand connections as your Python script
    const HAND_CONNECTIONS = [
      [0, 1], [1, 2], [2, 3], [3, 4],
      [0, 5], [5, 6], [6, 7], [7, 8],
      [0, 9], [9, 10], [10, 11], [11, 12],
      [0, 13], [13, 14], [14, 15], [15, 16],
      [0, 17], [17, 18], [18, 19], [19, 20],
      [5, 9], [9, 13], [13, 17]
    ];

    // ===== DOM ELEMENTS =====
    const videoEl = document.getElementById("video");
    const canvasEl = document.getElementById("output-canvas");
    const ctx = canvasEl.getContext("2d");

    const letterEl = document.getElementById("prediction-letter");
    const confEl   = document.getElementById("prediction-confidence");
    const handStatusEl = document.getElementById("hand-status");
    const fpsEl    = document.getElementById("fps");
    const probContainer = document.getElementById("probabilities");
    const loadingOverlay = document.getElementById("loading-overlay");

    // Canvas to hold the mirrored frame (equivalent to cv2.flip(frame, 1))
    const mirrorCanvas = document.createElement("canvas");
    const mirrorCtx = mirrorCanvas.getContext("2d");

    // Canvas for cropped input to the model
    const cropCanvas = document.createElement("canvas");
    cropCanvas.width = IMG_SIZE;
    cropCanvas.height = IMG_SIZE;
    const cropCtx = cropCanvas.getContext("2d");

    let model = null;
    let lastFrameTime = performance.now();
    let lastPredictionTime = 0;
    const PRED_INTERVAL_MS = 120;

    async function loadModel() {
      model = await tf.loadGraphModel(MODEL_URL);
      console.log("TF.js model loaded");
    }

    function drawWireframe(landmarks, width, height) {
      // draw connections
      ctx.strokeStyle = "#ffffff";
      ctx.lineWidth = 2;
      for (const [start, end] of HAND_CONNECTIONS) {
        const s = landmarks[start];
        const e = landmarks[end];
        const x1 = s.x * width;
        const y1 = s.y * height;
        const x2 = e.x * width;
        const y2 = e.y * height;
        ctx.beginPath();
        ctx.moveTo(x1, y1);
        ctx.lineTo(x2, y2);
        ctx.stroke();
      }

      // draw points
      ctx.fillStyle = "#ff0000";
      for (const lm of landmarks) {
        const x = lm.x * width;
        const y = lm.y * height;
        ctx.beginPath();
        ctx.arc(x, y, 4, 0, Math.PI * 2);
        ctx.fill();
      }
    }

    function setupHands() {
      const hands = new Hands({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
      });

      hands.setOptions({
        maxNumHands: 1,
        modelComplexity: 1,
        minDetectionConfidence: 0.6,
        minTrackingConfidence: 0.5,
      });

      hands.onResults(onResults);
      return hands;
    }

    async function onResults(results) {
      const now = performance.now();
      const dt = now - lastFrameTime;
      lastFrameTime = now;
      fpsEl.textContent = "FPS: " + (1000 / dt).toFixed(1);

      // Match canvas size to mirrored frame
      if (canvasEl.width !== mirrorCanvas.width)
        canvasEl.width = mirrorCanvas.width;
      if (canvasEl.height !== mirrorCanvas.height)
        canvasEl.height = mirrorCanvas.height;

      // Draw the mirrored frame
      ctx.clearRect(0, 0, canvasEl.width, canvasEl.height);
      ctx.drawImage(mirrorCanvas, 0, 0, canvasEl.width, canvasEl.height);

      if (!results.multiHandLandmarks ||
          results.multiHandLandmarks.length === 0) {
        handStatusEl.textContent = "No hand detected";
        letterEl.textContent = "–";
        confEl.textContent = "–";
        return;
      }

      const landmarks = results.multiHandLandmarks[0];
      handStatusEl.textContent = "Hand detected";

      // Draw the wireframe exactly like your Python version
      drawWireframe(landmarks, canvasEl.width, canvasEl.height);

      // Bounding box from landmarks
      const xs = landmarks.map(lm => lm.x * canvasEl.width);
      const ys = landmarks.map(lm => lm.y * canvasEl.height);
      let xmin = Math.min(...xs);
      let xmax = Math.max(...xs);
      let ymin = Math.min(...ys);
      let ymax = Math.max(...ys);

      const pad = 20;
      xmin = Math.max(0, xmin - pad);
      ymin = Math.max(0, ymin - pad);
      xmax = Math.min(canvasEl.width,  xmax + pad);
      ymax = Math.min(canvasEl.height, ymax + pad);

      const boxW = xmax - xmin;
      const boxH = ymax - ymin;

      // Draw bounding box
      ctx.strokeStyle = "#22c55e";
      ctx.lineWidth = 2;
      ctx.strokeRect(xmin, ymin, boxW, boxH);

      if (!model) return;
      if (now - lastPredictionTime < PRED_INTERVAL_MS) return;
      lastPredictionTime = now;

      if (boxW <= 2 || boxH <= 2) return;

      // Crop from mirrored frame (so model sees exactly what Python saw)
      cropCtx.clearRect(0, 0, IMG_SIZE, IMG_SIZE);
      cropCtx.drawImage(
        mirrorCanvas,
        xmin, ymin, boxW, boxH,
        0, 0, IMG_SIZE, IMG_SIZE
      );

      // MODEL INPUT: normalize exactly like live_predict.py
      tf.engine().startScope();
      try {
        let img = tf.browser.fromPixels(cropCanvas).toFloat().div(255.0); // [H,W,3]

        const offset = tf.tensor1d(NORM_MEAN).reshape([1,1,3]);
        const scale  = tf.tensor1d(NORM_STD ).reshape([1,1,3]);
        img = img.sub(offset).div(scale);

        const input = img.expandDims(0); // [1,256,256,3]

        const logits = model.execute({ "input": input }); // [1,24]
        const probsTensor = tf.softmax(logits);
        const probs = probsTensor.dataSync(); // Float32Array

        // Argmax
        let maxIdx = 0;
        let maxVal = probs[0];
        for (let i = 1; i < probs.length; i++) {
          if (probs[i] > maxVal) {
            maxVal = probs[i];
            maxIdx = i;
          }
        }

        const letter = CLASS_NAMES[maxIdx] || "?";
        const confidence = maxVal * 100;

        letterEl.textContent = letter;
        confEl.textContent = confidence.toFixed(1) + "%";

        updateProbabilities(probs);
      } catch (err) {
        console.error("Inference error:", err);
        handStatusEl.textContent = "Error during prediction (see console)";
      } finally {
        tf.engine().endScope();
      }
    }

    function updateProbabilities(probs) {
      probContainer.innerHTML = "";
      for (let i = 0; i < CLASS_NAMES.length; i++) {
        const row = document.createElement("div");
        row.className = "bar-row";

        const label = document.createElement("div");
        label.className = "bar-label";
        label.textContent = CLASS_NAMES[i];

        const barBg = document.createElement("div");
        barBg.className = "bar-bg";

        const barFill = document.createElement("div");
        barFill.className = "bar-fill";
        barFill.style.width = (probs[i] * 100).toFixed(1) + "%";

        barBg.appendChild(barFill);

        const val = document.createElement("div");
        val.className = "bar-val";
        val.textContent = (probs[i] * 100).toFixed(1) + "%";

        row.appendChild(label);
        row.appendChild(barBg);
        row.appendChild(val);

        probContainer.appendChild(row);
      }
    }

    async function init() {
      try {
        handStatusEl.textContent = "Loading model. This could take a minute…";
        await loadModel();

        // Start camera
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { width: 640, height: 480 },
          audio: false
        });
        videoEl.srcObject = stream;

        await new Promise(res => videoEl.onloadedmetadata = res);
        videoEl.play();

        // Match mirror canvas to video size
        mirrorCanvas.width = videoEl.videoWidth;
        mirrorCanvas.height = videoEl.videoHeight;

        const hands = setupHands();

        // Start MediaPipe camera processing with mirrored input
        const camera = new Camera(videoEl, {
          onFrame: async () => {
            // Mirror frame: equivalent of cv2.flip(frame, 1)
            mirrorCtx.save();
            mirrorCtx.scale(-1, 1);
            mirrorCtx.drawImage(
              videoEl,
              -videoEl.videoWidth, 0,
              videoEl.videoWidth, videoEl.videoHeight
            );
            mirrorCtx.restore();

            await hands.send({ image: mirrorCanvas });
          },
          width: videoEl.videoWidth,
          height: videoEl.videoHeight,
        });

        camera.start();

        handStatusEl.textContent = "Running — show your hand!";
      } catch (err) {
        console.error(err);
        handStatusEl.textContent = "Failed to initialize (see console)";
      } finally {
        // Hide loading overlay once model + camera are set up
        loadingOverlay.style.display = "none";
      }
    }

    window.addEventListener("load", () => {
      init();
    });
  </script>
</body>
</html>
